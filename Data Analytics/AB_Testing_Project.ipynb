{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing\n",
    "#### P7:  Udacity Data Analyst Nanodegree\n",
    "#### Author:  Whitney King (with excerpts from Udacity project template) \n",
    "#### Date:  8/31/2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into 'C:/Users/dooki/Documents/R/win-library/3.3'\n",
      "(as 'lib' is unspecified)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'ggplot2' successfully unpacked and MD5 sums checked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"unable to move temporary installation 'C:\\Users\\dooki\\Documents\\R\\win-library\\3.3\\fileaf5826e1712\\ggplot2' to 'C:\\Users\\dooki\\Documents\\R\\win-library\\3.3\\ggplot2'\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\dooki\\AppData\\Local\\Temp\\RtmpiKqroK\\downloaded_packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.3.3\"Installing package into 'C:/Users/dooki/Documents/R/win-library/3.3'\n",
      "(as 'lib' is unspecified)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'rmarkdown' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\dooki\\AppData\\Local\\Temp\\RtmpiKqroK\\downloaded_packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'rmarkdown' was built under R version 3.3.3\""
     ]
    }
   ],
   "source": [
    "# Load packages and settings\n",
    "install.packages('ggplot2', repos = \"http://cran.us.r-project.org\")\n",
    "library(ggplot2)\n",
    "\n",
    "install.packages('rmarkdown', repos = \"http://cran.us.r-project.org\")\n",
    "library(rmarkdown)\n",
    "\n",
    "options(\"scipen\"=100, \"digits\"=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Experiment Design\n",
    "\n",
    "### Overview\n",
    "\n",
    "When this experiment was conducted, Udacity courses offer two options to visitors on the homepage: \n",
    "\n",
    "1. Start Free Trial\n",
    "  * If a student clicks *start free trial*, they will be prompted to enter payment information. Once entered, they will be enrolled into a free trial of their course program which will provide them full access to the course materials for 14 days, after which point they will be billed for the tuition if they don't opt to cancel the trial.\n",
    "2. Access Course Materials\n",
    "  * If a student clicks *access course materials*, they will be able to immediately view the course videos and take the quizzes for free, however they will not be given coaching support, a verified certificate, and will not receive feedback on their final projects.\n",
    "\n",
    "In the experiment, Udacity tested the following change where if the student clicked *start free trial*, they were asked how much time they had available to devote to the course:\n",
    "\n",
    "1. If the student indicated 5 or more hours per week, they would be taken through the checkout process as usual. \n",
    "2. If they indicated fewer than 5 hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion, and suggesting that the student might like to access the course materials for free. \n",
    "  * At this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "\n",
    "Hypothesis testing should be done to determine the probability of the experiment results occurring by chance. A standard null hypothesis would be that there is no significant difference between the results of the control and experiment groups.\n",
    "\n",
    "The hypothesis was that this might set clearer expectations for students upfront, thus:\n",
    "\n",
    "1. Reducing the number of frustrated students who left the free trial because they didn't have enough time\n",
    "2. Without significantly reducing the number of students to continue past the free trial and eventually complete the course. \n",
    "\n",
    "If this hypothesis held true, Udacity could improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course.\n",
    "\n",
    "The unit of diversion is a cookie, although if the student enrolls in the free trial, they are tracked by user-id from that point forward. The same user-id cannot enroll in the free trial twice. For users that do not enroll, their user-id is not tracked in the experiment, even if they were signed in when they visited the course overview page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Choice\n",
    "\n",
    "#### Invariant Metrics\n",
    "\n",
    " * **Number of Cookies**: Number of unique cookies to view course overview page\n",
    "  * The goal is to split cookies evenly between the control and experiment groups, and cannot be an evaluation metric\n",
    "  * We don't expect to see a lot of variation between the two for this number, so it is an invariant metric\n",
    "\n",
    "\n",
    " * **Number of Clicks**: Number of unique cookies to click the *Start Free Trial* button (which happens before the free trial screener is triggered)\n",
    "  * The scope of the experiment begins after the user has clicked the *Start Free Trial* button\n",
    "  * The number of unique cookies that clicked should be relatively evenly distributed between the control and experiment groups, so is an invariant metric and cannot be an evaluation metric\n",
    "\n",
    "\n",
    " * **Click-Through-Probability**: Number of unique cookies to click the *Start Free Trial* button divided by number of unique cookies to view the course overview page\n",
    "  * Click-Through-Probability should have slight variation between the control and experiment groups, and cannot be an evaluation metric\n",
    "  * We don't expect to see a lot of variation between the two for this number, so it is an invariant metric\n",
    "\n",
    "#### Evaluation Metrics\n",
    "\n",
    " * **Gross Conversion**: Number of user-ids to complete checkout and enroll in free trial divided by number of unique cookies to click \"Start Free Trial\" button\n",
    "  * This metric will tell us if bullet point 1 of our hypothesis holds up (reduce number of students that leave the free trial due to lack of time to complete the coursework)\n",
    "  * If this part of the hypothesis is true, we should be able to measure a statistically significant reduction in the Gross Conversion of the *experiment* group\n",
    "  \n",
    "  \n",
    " * **Net Conversion**: Number of user-ids to remain enrolled past the 14-day boundary divided by the number of unique cookies to click the \"Start Free Trial\" button\n",
    "  * This metric will tell us if bullet point 2 of our hypothesis holds up (does not significantly reduce the number of students that continue past the free trial and complete the course)\n",
    "  * If this part of the hypothesis is true, we should *not* be able to measure a statistically significant reduction of Net Conversion in the experiment group (i.e. it can be the same or higher than the control group)\n",
    "\n",
    "#### Unused Metrics\n",
    "\n",
    " * **Number of user-ids**: Number of users who enroll in the free trial\n",
    "  * This could have been chosen as an evaluation metric, however in this situation Gross Conversion was a better measure for the goals of this A/B test\n",
    "  \n",
    "  \n",
    " * **Retention**: Number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by number of users to enroll in the free trial\n",
    "   * This number uses different units for the analysis and diversion, which can lead to inaccuracies\n",
    "   * The number is expected to change, and wouldn't be an invariant metric\n",
    "   * Gross Conversion and Net Conversion already provide us the framework we need for testing our hypothesis\n",
    "   * Getting enough data to reliably measure retention can be a very time intensive process, and doesn't lend itself well to being chosen as an evaluation metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Standard Deviation\n",
    "\n",
    "For each evaluation metric, standard deviation should be calculated. **Gross Conversion** and **Net Conversion** are measured as probabilities, so we can expect a normal, binomial distribution if the sample size is sufficient. The [baseline values](https://docs.google.com/spreadsheets/d/1MYNUtC47Pg8hdoCjOXaHqF-thheGpUshrFA21BAJnNc/edit#gid=0) are as follows, and we can guesstimate we'll need about 5000 cookies per day in each group for the experiment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_size <- 5000                    # Size of total sample of cookie visits being used in A/B Test\n",
    "cookies.view <- 40000                  # Unique cookies to view page per day\n",
    "cookies.click <- 3200                  # Unique cookies to click \"Start free trial\" per day\n",
    "enrollments <- 660                     # Enrollments per day\n",
    "click_thru_prob <- .08                 # Click-through-probability on \"Start free trial\"\n",
    "enroll_prob_with_click <- .20625       # Probability of enrolling, given click (base conversion rate)\n",
    "pay_prob_with_enroll <- .53            # Probability of payment, given enroll\n",
    "pay_prob_with_click <- .1093125        # Probability of payment, given click\n",
    "\n",
    "click_sample <- sample_size * click_thru_prob    # N = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the standard deviation of the evaluation metrics with an assumed binomial distribution, we can use the formula:\n",
    "\n",
    "![SD = sqrt(p(1-p)/N)](https://latex.codecogs.com/png.latex?SD%20%3D%20%5Csqrt%7B%5Cfrac%7Bp*%281-p%29%7D%7BN%20%7D%7D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gross Conversion Standard Deviation:   0.0202 \n",
      "Net Conversion Standard Deviation:   0.0156"
     ]
    }
   ],
   "source": [
    "gross_converison.SD <- round(sqrt(enroll_prob_with_click*(1-enroll_prob_with_click)/click_sample), digits = 4)\n",
    "net_conversion.SD <- round(sqrt(pay_prob_with_click*(1-pay_prob_with_click)/click_sample), digits = 4)\n",
    "\n",
    "cat('Gross Conversion Standard Deviation:  ', gross_converison.SD, '\\n')\n",
    "cat('Net Conversion Standard Deviation:  ', net_conversion.SD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gross Conversion** and **Net Conversion** are both calculated using the **Number of Cookies** metric as the unit of diversion. This metric is also the unit of analysis for both gross and net conversion, so estimates should be fairly consistent with empirical variability and can be used for further investigation down the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sizing\n",
    "\n",
    "#### Number of Samples vs. Power\n",
    "Since **Gross Conversion** and **Net Conversion** are already correlated, Bonferroni Correction was not used in the calculations to avoid increasing the number of potential false positives.\n",
    "\n",
    "Using the sample size calculator in [Evan's Awesome A/B Tools](https://www.evanmiller.org/ab-testing/sample-size.html), we can determine the following sample sizes for **Gross Conversion** and **Net Conversion**:\n",
    "\n",
    "* **Gross Conversion**\n",
    "  * Baseline:  .20625\n",
    "  * dMin: .01\n",
    "  * α: .05\n",
    "  * 1−β: .8\n",
    "  * **Sample Size**: 25,835\n",
    "  \n",
    "  \n",
    "* **Net Conversion**:\n",
    "  * Baseline:  .1093125\n",
    "  * dMin: .0075\n",
    "  * α: .05\n",
    "  * 1−β: .8\n",
    "  * **Sample Size**: 27,413\n",
    "  \n",
    "Since **Net Conversion** requires the larger sample size, we'll take a closer look at that one. We can use the previously calculated click-through-probability to get the total number of pageviews we should have for each group:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pageviews Per Group:   342663 \n",
      "Total Pageviews:   685325 \n"
     ]
    }
   ],
   "source": [
    "gross_conversion.size <- 25835\n",
    "net_conversion.size <- 27413\n",
    "\n",
    "pageviews.group <- net_conversion.size / click_thru_prob\n",
    "cat('Pageviews Per Group:  ', pageviews.group, '\\n')\n",
    "\n",
    "pageviews.total = pageviews.group * 2\n",
    "cat('Total Pageviews:  ', pageviews.total, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duration vs. Exposure\n",
    "\n",
    "Given that there are 40,000 view cookies each day, and we'll require 685,325 pageviews the two groups in the experiment, it can be expected that it will take a period to collect the data required for the two experiment groups. When the impact of the parameters involved in this experiment are considered, the risk to the user is minimal and it shouldn't be a deterrent to any current students. The data being evaluated doesn't pose any overt risks to the users, and doesn't gather sensitive information about them.\n",
    "\n",
    "Diverting most of traffic to the experiment should be fine, but it's a best practice to not divert all traffic. Since the data would take minimally a few weeks to collect, a good goal would be in keeping the data collection time under a month. With these considerations, I would divert roughly 60% of site traffic to the experiment groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 % of Traffic: 24000 \n",
      "Days of Collection:   29"
     ]
    }
   ],
   "source": [
    "traffic.percent <- .6\n",
    "traffic.divert <- cookies.view * traffic.percent\n",
    "traffic.days <- round(pageviews.total / traffic.divert, digits=0)\n",
    "\n",
    "cat(traffic.percent*100, '% of Traffic:', traffic.divert, '\\n')\n",
    "cat('Days of Collection:  ', traffic.days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Analysis\n",
    "\n",
    "### Sanity Checks\n",
    "\n",
    "The data for the experiment was obtained from the [spreadsheet provided by Udacity](https://docs.google.com/spreadsheets/d/1Mu5u9GrybDdska-ljPXyBjTpdZIUev_6i7t4LRDfXM8/edit#gid=0). To begin, we'll calculate totals for cookies views, clicks, and click-through-probability. It's expected that the experiment and control groups should be relatively even. Then, we'll calculate the confidence interval, and upper and lower bounds for each invariant metric, and whether those values pass the sanity checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie Views: \n",
      "     Control Cookies:   345543 \n",
      "     Experiment Cookies:   344660 \n",
      "     Total Cookies:   690203 \n",
      "     Expected Probability:  0.5 \n",
      "     ------------------------------- \n",
      "     Observed Probability:   0.5006 \n",
      "     Sigma: 0.0006018 \n",
      "     ME: 0.00118 \n",
      "     CI: [ 0.4988 , 0.5012 ] \n",
      "     Pass: TRUE \n",
      "\n",
      "\n",
      "Clicks: \n",
      "     Control Clicks:   28378 \n",
      "     Experiment Clicks:   28325 \n",
      "     Total Clicks:   56703 \n",
      "     Expected Probability:  0.5 \n",
      "     ------------------------------- \n",
      "     Observed Probability:   0.5005 \n",
      "     Sigma: 0.0021 \n",
      "     ME: 0.004116 \n",
      "     CI: [ 0.4959 , 0.5041 ] \n",
      "     Pass: TRUE \n",
      "\n",
      "\n",
      "Click-Through-Probability: \n",
      "     Control CTP:   0.0821 \n",
      "     Experiment CTP:   0.0822 \n",
      "     Expected CTP:  0.0821 \n",
      "     ------------------------------- \n",
      "     Pooled CTP:   0.08215 \n",
      "     CTP Difference:   0.0001 \n",
      "     Sigma:   0.0004671 \n",
      "     ME:   0.0009156 \n",
      "     CI: [ 0.08118 , 0.08302 ] \n",
      "     Pass: TRUE"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "control <- read.csv(\"control.csv\", header = T, check.names = F,\n",
    "                    na.strings = c(\"\"))\n",
    "\n",
    "experiment <- read.csv(\"experiment.csv\", header = T, check.names = F,\n",
    "                    na.strings = c(\"\"))\n",
    "\n",
    "p <- .5\n",
    "z_score <- 1.96\n",
    "\n",
    "control.pageviews <- sum(control$Pageviews)\n",
    "experiment.pageviews <- sum(experiment$Pageviews)\n",
    "cookies.total <- control.pageviews + experiment.pageviews\n",
    "cookies.prob <- control.pageviews / cookies.total\n",
    "cookies.sigma <- sqrt((p*(1-p))/(control.pageviews+experiment.pageviews))\n",
    "cookies.me <- z_score * cookies.sigma\n",
    "cookies.lower <- c(p-cookies.me)\n",
    "cookies.upper <- c(p+cookies.me)\n",
    "cookies.pass <- cookies.lower < cookies.prob && cookies.prob < cookies.upper\n",
    "\n",
    "control.clicks <- sum(control$Clicks)\n",
    "experiment.clicks <- sum(experiment$Clicks)\n",
    "total.clicks <- control.clicks + experiment.clicks\n",
    "click.prob <- control.clicks / total.clicks\n",
    "click.sigma <- sqrt((p*(1-p))/(control.clicks + experiment.clicks))\n",
    "click.me <- z_score * click.sigma\n",
    "click.lower <- c(p-click.me)\n",
    "click.upper <- c(p+click.me)\n",
    "click.pass <- click.lower < click.prob && click.prob < click.upper\n",
    "\n",
    "control.ctp <- round(control.clicks / control.pageviews, digits=4)\n",
    "experiment.ctp <- round(experiment.clicks / experiment.pageviews, digits=4)\n",
    "ctp.pool <- (control.ctp + experiment.ctp) / 2\n",
    "ctp.diff <- experiment.ctp - control.ctp\n",
    "ctp.sigma <- sqrt((ctp.pool*(1-ctp.pool))/control.pageviews)\n",
    "ctp.me <- z_score * ctp.sigma\n",
    "ctp.lower <- c(control.ctp-ctp.me)\n",
    "ctp.upper <- c(control.ctp+ctp.me)\n",
    "ctp.pass <- ctp.lower < ctp.pool && ctp.pool < ctp.upper\n",
    "\n",
    "cat('Cookie Views: \\n',\n",
    "    '    Control Cookies:  ', control.pageviews, '\\n',\n",
    "    '    Experiment Cookies:  ', experiment.pageviews, '\\n',\n",
    "    '    Total Cookies:  ', cookies.total, '\\n',\n",
    "    '    Expected Probability:  0.5', '\\n',\n",
    "    '    -------------------------------', '\\n',\n",
    "    '    Observed Probability:  ', cookies.prob, '\\n',\n",
    "    '    Sigma:', cookies.sigma, '\\n',\n",
    "    '    ME:', cookies.me, '\\n',\n",
    "    '    CI:', '[', cookies.lower, ',', cookies.upper, ']', '\\n',\n",
    "    '    Pass:', cookies.pass, '\\n\\n\\n')\n",
    "\n",
    "cat('Clicks: \\n',\n",
    "    '    Control Clicks:  ', control.clicks, '\\n', \n",
    "    '    Experiment Clicks:  ', experiment.clicks, '\\n',\n",
    "    '    Total Clicks:  ', total.clicks, '\\n',\n",
    "    '    Expected Probability:  0.5', '\\n',\n",
    "    '    -------------------------------', '\\n',\n",
    "    '    Observed Probability:  ', click.prob, '\\n',\n",
    "    '    Sigma:', click.sigma, '\\n',\n",
    "    '    ME:', click.me, '\\n',\n",
    "    '    CI:', '[', click.lower, ',', click.upper, ']', '\\n',\n",
    "    '    Pass:', click.pass, '\\n\\n\\n')\n",
    "\n",
    "cat('Click-Through-Probability: \\n',\n",
    "    '    Control CTP:  ', control.ctp, '\\n',\n",
    "    '    Experiment CTP:  ', experiment.ctp, '\\n',\n",
    "    '    Expected CTP:  0.0821', '\\n',\n",
    "    '    -------------------------------', '\\n',\n",
    "    '    Pooled CTP:  ', ctp.pool, '\\n',\n",
    "    '    CTP Difference:  ', ctp.diff, '\\n',\n",
    "    '    Sigma:  ', ctp.sigma, '\\n',\n",
    "    '    ME:  ', ctp.me, '\\n',\n",
    "    '    CI:', '[', ctp.lower, ',', ctp.upper, ']', '\\n',\n",
    "    '    Pass:', ctp.pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the sanity checks, all three invariant metrics that were chosen pass, and are good picks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis\n",
    "\n",
    "#### Effect Size Tests\n",
    "\n",
    "Now that the experiment has passed initial sanity checks, it's time to test the statistical and practical significance of the evaluation metrics using only complete rows of data. The evaluation metrics are considered statistically significant if 0 is not within the bounds of the confidence interval, which indicates there was a change between the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicks \n",
      "     Control: 17293 \n",
      "     Experiment: 17260 \n",
      "     Total: 34553 \n",
      "\n",
      "\n",
      "Enrollments: \n",
      "     Control: 3785 \n",
      "     Experiment: 3423 \n",
      "     Total: 7208 \n",
      "\n",
      "\n",
      "Payments: \n",
      "     Control: 2033 \n",
      "     Experiment: 1945 \n",
      "     Total: 3978 \n",
      "\n",
      "\n",
      "--------------------------------------------------- \n",
      "Gross Conversion: \n",
      "     Pooled: 0.2086 \n",
      "     Difference: -0.02055 \n",
      "     Sigma: 0.004372 \n",
      "     ME: 0.008568 \n",
      "     Practical Boundary: 0.01 \n",
      "     CI: [ -0.02912 , -0.01199 ] \n",
      "     Statistically Significant?:   YES \n",
      "     Practicall Significant?:   YES \n",
      "\n",
      "\n",
      "Net Conversion: \n",
      "     Pooled: 0.1151 \n",
      "     Difference: -0.004874 \n",
      "     Sigma: 0.003434 \n",
      "     ME: 0.006731 \n",
      "     Practical Boundary: 0.0075 \n",
      "     CI: [ -0.0116 , 0.001857 ] \n",
      "     Statistically Significant?:   NO \n",
      "     Practicall Significant?:   NO \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "control.clicks <- sum(control$Clicks[1:23])\n",
    "experiment.clicks <- sum(experiment$Clicks[1:23])\n",
    "total.clicks <- control.clicks + experiment.clicks\n",
    "\n",
    "control.enrollments <- sum(control$Enrollments[1:23])\n",
    "experiment.enrollments <- sum(experiment$Enrollments[1:23])\n",
    "total.enrollments <- control.enrollments + experiment.enrollments\n",
    "\n",
    "control.payments <- sum(control$Payments[1:23])\n",
    "experiment.payments <- sum(experiment$Payments[1:23])\n",
    "total.payments <- control.payments + experiment.payments\n",
    "\n",
    "gross_conversion.dmin <- .01\n",
    "gross_conversion.pool <- total.enrollments / total.clicks\n",
    "gross_conversion.diff <- (experiment.enrollments/experiment.clicks) - (control.enrollments/control.clicks)\n",
    "gross_conversion.sigma <- sqrt((gross_conversion.pool*(1-gross_conversion.pool))*((1/control.clicks)+(1/experiment.clicks)))\n",
    "gross_conversion.me <- z_score * gross_conversion.sigma\n",
    "gross_conversion.lower <- c(gross_conversion.diff-gross_conversion.me)\n",
    "gross_conversion.upper <- c(gross_conversion.diff+gross_conversion.me)\n",
    "\n",
    "net_conversion.dmin <- .0075\n",
    "net_conversion.pool <- total.payments / total.clicks\n",
    "net_conversion.diff <- (experiment.payments/experiment.clicks) -(control.payments/control.clicks)\n",
    "net_conversion.sigma <- sqrt((net_conversion.pool*(1-net_conversion.pool))*((1/control.clicks)+(1/experiment.clicks)))\n",
    "net_conversion.me <- z_score * net_conversion.sigma\n",
    "net_conversion.lower <- c(net_conversion.diff-net_conversion.me)\n",
    "net_conversion.upper <- c(net_conversion.diff+net_conversion.me)\n",
    "\n",
    "\n",
    "\n",
    "cat('Clicks', '\\n',\n",
    "    '    Control:', control.clicks, '\\n',\n",
    "    '    Experiment:', experiment.clicks, '\\n',\n",
    "    '    Total:', total.clicks, '\\n\\n\\n')\n",
    "\n",
    "cat('Enrollments:', '\\n',\n",
    "    '    Control:', control.enrollments, '\\n',\n",
    "    '    Experiment:', experiment.enrollments, '\\n',\n",
    "    '    Total:', total.enrollments, '\\n\\n\\n')\n",
    "\n",
    "cat('Payments:', '\\n',\n",
    "    '    Control:', control.payments, '\\n',\n",
    "    '    Experiment:', experiment.payments, '\\n',\n",
    "    '    Total:', total.payments, '\\n\\n\\n')\n",
    "\n",
    "cat('---------------------------------------------------', '\\n')\n",
    "cat('Gross Conversion: \\n',\n",
    "    '    Pooled:', gross_conversion.pool, '\\n',\n",
    "    '    Difference:', gross_conversion.diff, '\\n',\n",
    "    '    Sigma:', gross_conversion.sigma, '\\n',\n",
    "    '    ME:', gross_conversion.me, '\\n',\n",
    "    '    Practical Boundary:', gross_conversion.dmin, '\\n',\n",
    "    '    CI:', '[', gross_conversion.lower, ',', gross_conversion.upper, ']', '\\n',\n",
    "    '    Statistically Significant?:   YES', '\\n',\n",
    "    '    Practicall Significant?:   YES', '\\n\\n\\n')\n",
    "\n",
    "#prop.test(x=c(total.enrollments), n=c(total.clicks))\n",
    "\n",
    "cat('Net Conversion: \\n',\n",
    "    '    Pooled:', net_conversion.pool, '\\n',\n",
    "    '    Difference:', net_conversion.diff, '\\n',\n",
    "    '    Sigma:', net_conversion.sigma, '\\n',\n",
    "    '    ME:', net_conversion.me, '\\n',\n",
    "    '    Practical Boundary:', net_conversion.dmin, '\\n',\n",
    "    '    CI:', '[', net_conversion.lower, ',', net_conversion.upper, ']', '\\n',\n",
    "    '    Statistically Significant?:   NO', '\\n',\n",
    "    '    Practicall Significant?:   NO', '\\n\\n\\n')\n",
    "\n",
    "#prop.test(x=c(total.payments), n=c(total.clicks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Gross Conversion** *is* statistically significant since the interval does *not* include 0, and it is practicially significant because it does not pass the practical significance boundaries. This metric passes.\n",
    "\n",
    "* **Net Conversion** *is not* statistically *or* pracitically significant since the interval includes 0. This metrics doesn't pass. The confidence interval is beyond the boundary of practical significance, so we could interpret this data incorrectly. Due to these factors, this metrics doesn't pass.\n",
    "\n",
    "\n",
    "\n",
    "#### Sign Tests\n",
    "\n",
    "Two-tailed t-tests were run using the [Sign and Binomical Test](http://graphpad.com/quickcalcs/binomial2/) tool hosted by GraphPad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gross Conversion: \n",
      "     Successes:  4 \n",
      "     # Trials:  23 \n",
      "     Probability:  .5 \n",
      "     p-value:  .0026 \n",
      "     Statistically Significant?:  YES \n",
      "\n",
      "\n",
      "Net Conversion: \n",
      "     Successes:  10 \n",
      "     # Trials:  23 \n",
      "     Probability:  .5 \n",
      "     p-value:  .6776 \n",
      "     Statistically Significant?:  NO \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat('Gross Conversion: \\n',\n",
    "    '    Successes:  4', '\\n',\n",
    "    '    # Trials:  23', '\\n',\n",
    "    '    Probability:  .5', '\\n',\n",
    "    '    p-value:  .0026', '\\n',\n",
    "    '    Statistically Significant?:  YES', '\\n\\n\\n')\n",
    "\n",
    "cat('Net Conversion: \\n',\n",
    "    '    Successes:  10', '\\n',\n",
    "    '    # Trials:  23', '\\n',\n",
    "    '    Probability:  .5', '\\n',\n",
    "    '    p-value:  .6776', '\\n',\n",
    "    '    Statistically Significant?:  NO', '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "This experiment was divided into two groups: control and experiment. The control group was enrolled in a free trial version of their chosen course after clicking *start free trial*. Upon registration, they were asked how much time they expect to be able to devote to the learning each week. The control group was not taken through this trial flow. A null hypothesis was formed stating there was no significant difference in the evaluation metrics of the two different groups, and a practical significance was set for each of these metrics.\n",
    "\n",
    "There were a a handful of metrics to choose from, some of which were suited as invariant mertrics, and others more suited towards evaluation metrics. The invariant metrics chosen were **Number of Cookies**, **Number of Clicks**, and **Click-Through-Probability**. The evaluation metrics chosen were **Gross Conversion**, and **Net Conversion**. Due to evaluation metrics already being correlated, the Bonferroni Correction was not used. Using the Bonferroni Correction in this case would have increased the statistical error, as well as the number of false negatives. For this experiment, all metrics needed to be met, and false negatives could impact the decision to launch. \n",
    "\n",
    "After sanitity checks were ran, and the results of the experiement were analyzed, the experiment group experienced a statistically significant decrease in **Gross Conversion** when compared to the control group with a 95% CI. These findings aligned with the hypothesis. However, in the case **Net Conversion**, the results were *not* found to be statistically significant, and the confidence interval exceeded the negative end of the practical significance boundary. Both effect and size tests returned the same findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation\n",
    "\n",
    "The experiments design was built around gaining an understanding of if filtering students using stated study time commitment would improve the overall student experience, without negatively impacting the number of students that continue through the trial period and into full enrollment. We were able to determine there were both statistically and practically significant differences between the two groups when it came to **Gross Conversion**, however there were no significant differences with **Net Conversion**, and the confidence interval exceed the negative practical significance boundary. What this means is that we saw a *decrease* in enrollment that was accompanied by no differences in conversions to enrollment from the trial period, so this experiment was not effective in accomplishing it's goals. \n",
    "\n",
    "Given these findings, the reccomendation would be to not launch this change, and to instead look into other options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow-Up Experiment\n",
    "\n",
    "The previous experiment failed given the goals stated at the beginning. Considering the way these results went, there are other options based on the idea of converting users to trial, and trials to enrollments that could make potentially interesting follow up experiments. Focusing the flow around volunteered information is maybe not the best way to select which users are shuffled into which flow, since sometimes people can be extremely off with their estimatations around things like dedication to study time.\n",
    "\n",
    "Perhaps trying a number of different trial periods (such as a month long free trial) versus the standard 2-week free trial would help calm insecurities students may have about not having enough time to really get a feel for how the program will go, and if it will be a good fit for them in the long run. Instead of having a system built around the user inputting information, I'd simply have the flow divert traffic to the experiment samples from any users that clicked *start free trial*.\n",
    "\n",
    "* **Null Hypothesis**: having access to a longer free trial period will not increase the number of students that enroll after the free trial period\n",
    "* **Unit of Diversion**: user-id, since we'll be tracking a change that occurs on a per-user-base after enrollment\n",
    "* **Invariant Metrics**:  user-id\n",
    "* **Evaluation Metrics**: Enrollments (count of number of enrollments), keeping a single evaluation metric will keep the SE at a minimum for this experiment, and it doesn't really need another one to measure the goal\n",
    "\n",
    "If we see statistically significant change is seen in the evaluation metrics, the experiment could be launched."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
